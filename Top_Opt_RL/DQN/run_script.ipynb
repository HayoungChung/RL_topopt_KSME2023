{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This is the main script to run the DQN algorithm. '''\n",
    "import os, sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(\"run_script.ipynb\"))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary modules\n",
    "from Top_Opt_RL.DQN.FEA_SOLVER_GENERAL import *\n",
    "from Top_Opt_RL.DQN.opts import parse_opts\n",
    "from Top_Opt_RL.DQN.TopOpt_Env_Functions import TopOpt_Gen, Prog_Refine_Act,User_Inputs,App_Inputs, Testing_Inputs, Testing_Info, Min_Dist_Calc  \n",
    "from Top_Opt_RL.DQN.Matrix_Transforms import obs_flip, action_flip, Mesh_Transform, Mesh_Triming \n",
    "from Top_Opt_RL.DQN.RL_Necessities import Agent \n",
    "from Plot_Layout import plot_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function to plot learning curve\n",
    "dependancy: matplotlib\n",
    "input: x (Episode number), y (Average Reward), filename (name of the file to save the plot)\n",
    "output: none\n",
    "'''\n",
    "def plot_learning_curve(x, scores, filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots()\n",
    "    running_average = np.zeros(len(scores))\n",
    "    for i in range(len(scores)):\n",
    "        running_average[i] = np.mean(scores[max(0, i-50):(i+1)])\n",
    "    \n",
    "    plt.title('Running average of previous 100 scores')    \n",
    "    ax.plot(x, running_average, color=\"blue\")\n",
    "    ax.set_xlabel(\"Episodes\", color=\"black\")\n",
    "    ax.set_ylabel(\"Average Reward\", color=\"black\")\n",
    "    ax.tick_params(axis='x', colors=\"black\")\n",
    "    ax.tick_params(axis='y', colors=\"black\")\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "# test of the plot_learning_curve function\n",
    "# x = [i+1 for i in range(100)]\n",
    "# y = [i+1 for i in range(100)]\n",
    "# plot_learning_curve(x, y, \"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This function stores the data history \n",
    "'''\n",
    "def Data_History(score_history, per_history, succ_history, Loss_history, Total_Loss, score, Main_EX, Main_EY,i):\n",
    "\n",
    "    Loss_history.append(Total_Loss)\n",
    "    avg_Loss=np.mean(Loss_history[-50:])\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-50:])\n",
    "    Succ_Steps=list(env.VoidCheck).count(0)\n",
    "    succ_history.append(Succ_Steps)\n",
    "\n",
    "    avg_succ = np.mean(succ_history[-50:])\n",
    "    Percent_Succ=Succ_Steps/(Main_EX*Main_EY)\n",
    "    per_history.append(Percent_Succ)\n",
    "    avg_percent=np.mean(per_history[-50:])\n",
    "    return score_history,per_history,succ_history,Loss_history,Succ_Steps,Percent_Succ,avg_succ,avg_score,avg_Loss,avg_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This is the main topopt function\n",
    "'''\n",
    "\n",
    "def TopOpt_Designing(User_Conditions, opts, envs): #,my_call_back_functions):\n",
    "    Time_Trial = opts.Time_Trial\n",
    "    if opts.Progressive_Refinement:\n",
    "        agent_primer= Agent(envs.env_primer,opts,Increase=False,filename_save=opts.filename_save+str(opts.PR_EX)+'by'+str(opts.PR_EY),\n",
    "                            filename_load=opts.filename_load,EX=opts.PR_EX,EY=opts.PR_EY, n_actions=opts.PR_EX*opts.PR_EY,\n",
    "                            epsilon=0,input_dims=[opts.PR_EX,opts.PR_EY,3])\n",
    "                            \n",
    "        agent_primer2= Agent(envs.env_primer2,opts,Increase=False,filename_save=opts.filename_save+str(opts.PR2_EX)+'by'+str(opts.PR2_EY),\n",
    "                            filename_load=opts.filename_load,EX=opts.PR2_EX,EY=opts.PR2_EY, n_actions=opts.PR2_EX*opts.PR2_EY, \n",
    "                            epsilon=0,input_dims=[opts.PR2_EX,opts.PR2_EY,3])\n",
    "        agent_primer.load_models()\n",
    "        agent_primer2.load_models()\n",
    "    \n",
    "    agent = Agent(envs.env,opts,Increase=False,filename_save=opts.filename_save+str(opts.Main_EX)+'by'+str(opts.Main_EY),\n",
    "                  filename_load=opts.filename_load,EX=opts.Main_EX,EY=opts.Main_EY, n_actions=opts.Main_EX*opts.Main_EY, \n",
    "                  epsilon=1.0, input_dims=[opts.Main_EX,opts.Main_EY,3])\n",
    "    if opts.Load_Checkpoints: agent.load_models()    \n",
    "    figure_file = 'plots/' + opts.filename_save +'_reward.png'    \n",
    "    best_score = envs.env.reward_range[0]    \n",
    "    score_history ,per_history,succ_history,Loss_history= [],[],[],[]\n",
    "    \n",
    "    if not opts.Load_Checkpoints:\n",
    "        from pandas import DataFrame \n",
    "        TrialData=DataFrame(columns=['Episode','Reward','Successfull Steps','Percent Successful','Avg Loss','SDEV','Epsilon','Time'])\n",
    "    envs.env.reset_conditions()\n",
    "    if opts.From_App:  opts.n_games=1\n",
    "    for i in range(opts.n_games):\n",
    "        Testing = False #Used to render the environment and track learning of the agent \n",
    "        if opts.Load_Checkpoints:\n",
    "            'If the user wants to test the agent, the user will be prompted to input BC and LC elements'\n",
    "            if opts.From_App:  App_Inputs(envs.env,envs.env_primer,envs.env_primer2,opts,User_Conditions)\n",
    "\n",
    "            else:  User_Inputs(envs.env,opts)\n",
    "\n",
    "        done = False\n",
    "        score = 0    \n",
    "        if i%10==0 and i>=100:\n",
    "            Testing=True\n",
    "            if i%200==0:\n",
    "                'Every 200 episodes, a special BC/LC will be used for monitoring purposes'\n",
    "                Testing_Inputs(envs.env,opts)\n",
    "                print('--------Testing Run------')\n",
    "        envs.env.VoidCheck=list(np.ones((1,envs.env.EX*envs.env.EY))[0])\n",
    "        if Time_Trial:     Start_Time_Trial=time.perf_counter()\n",
    "        observation = envs.env.reset()\n",
    "        print(envs.env)\n",
    "        if opts.Progressive_Refinement:\n",
    "            ''' Set Up to Complete 3 Iterations of Progressive Refinement'''\n",
    "            #Progressive Refinement #1 Going from Smallest to Intermediate Mesh Size\n",
    "            envs.env_primer.VoidCheck=list(np.ones((1,envs.env_primer.EX*envs.env_primer.EY))[0])\n",
    "            Prog_Refine_Act(agent_primer,envs.env,envs.env_primer,opts.Load_Checkpoints,Testing,opts,opts.PR_EX,opts.PR_EY,Time_Trial,opts.From_App,FEA_Skip=1)\n",
    "            #Progressive Refinement #2 Going for Intermediate to Final Mesh Size\n",
    "            envs.env_primer2.VoidCheck=Mesh_Transform(opts.PR_EX,opts.PR_EY,opts.PR2_EX,opts.PR2_EY,envs.env_primer.VoidCheck)\n",
    "            if opts.From_App:\n",
    "                del agent_primer\n",
    "            Prog_Refine_Act(agent_primer2,envs.env,envs.env_primer2,opts.Load_Checkpoints,Testing,opts,opts.PR2_EX,opts.PR2_EY,Time_Trial,opts.From_App,FEA_Skip=1)\n",
    "            #This outcome will now be used as the final mesh Size \n",
    "            envs.env.VoidCheck=Mesh_Transform(opts.PR2_EX,opts.PR2_EY,opts.Main_EX,opts.Main_EY,envs.env_primer2.VoidCheck)\n",
    "            if opts.From_App:\n",
    "                del agent_primer2\n",
    "            #Removed_Num=Mesh_Triming(env_primer,PR_EX,PR_EY)\n",
    "            #Uncomment the above line if you want to incorporate mesh trimming\n",
    "\n",
    "            observation[:,:,0]=np.reshape(FEASolve(envs.env.VoidCheck,opts.Lx,opts.Ly,opts.Main_EX,opts.Main_EY,envs.env.LC_Nodes,envs.env.Load_Directions,envs.env.BC_Nodes,Stress=True)[3],(opts.Main_EX,opts.Main_EY))\n",
    "        observation_v, observation_h,observation_vh=obs_flip(observation,opts.Main_EX,opts.Main_EY)\n",
    "        Last_Reward=0\n",
    "        while not done:\n",
    "            if i%1000==0 and i>=1: #Every 1000 iterations, show the activation maps\n",
    "                from keract import get_activations, display_activations \n",
    "                activations = get_activations(agent.q_eval.model, observation.reshape(-1,opts.Main_EX,opts.Main_EY,3))\n",
    "                display_activations(activations, save=False)\n",
    "            action = agent.choose_action(observation,opts.Load_Checkpoints,Testing)\n",
    "            observation_, reward, done, It= envs.env.step(action,observation,Last_Reward,opts.Load_Checkpoints,envs.env,FEA_Skip=1,PR=False)\n",
    "            if not opts.Load_Checkpoints:\n",
    "                observation_v_,observation_h_,observation_vh_=obs_flip(observation_,opts.Main_EX,opts.Main_EY)\n",
    "                action_v,action_h,action_vh=action_flip(action,opts.Main_EX,opts.Main_EY)\n",
    "                agent.store_transition(observation,action,reward,observation_,done)\n",
    "                agent.store_transition(observation_v,action_v,reward,observation_v_,done)\n",
    "                agent.store_transition(observation_h,action_h,reward,observation_h_,done)\n",
    "                agent.store_transition(observation_vh,action_vh,reward,observation_vh_,done)\n",
    "            score += reward\n",
    "            App_Plot=Testing_Info(envs.env,envs.env_primer,envs.env_primer2,opts,score,opts.Progressive_Refinement,opts.From_App,Fixed=True)\n",
    "            # _=[fn(App_Plot) for fn in my_call_back_functions]\n",
    "            Last_Reward=reward\n",
    "            if Testing and not Time_Trial:\n",
    "                envs.env.render()\n",
    "                print('Current Score: '+str(round(score,3)))\n",
    "            observation = observation_\n",
    "            if not opts.Load_Checkpoints:\n",
    "                observation_v=observation_v_\n",
    "                observation_h=observation_h_\n",
    "                observation_vh=observation_vh_\n",
    "            if opts.Load_Checkpoints and not Time_Trial:   envs.env.render()\n",
    "        App_Plot=Testing_Info(envs.env,envs.env_primer,envs.env_primer2,opts,score,opts.Progressive_Refinement,opts.From_App,Fixed=True)\n",
    "        # _=[fn(App_Plot) for fn in my_call_back_functions]\n",
    "        return App_Plot        \n",
    "        toc=time.perf_counter()\n",
    "\n",
    "        if Time_Trial and not opts.From_App:\n",
    "            print('It took '+str(round(toc-Start_Time_Trial,1))+' seconds to complete this time trial.')    \n",
    "\n",
    "        App_Plot=Testing_Info(envs.env,envs.env_primer,envs.env_primer2,opts,score,opts.Progressive_Refinement,opts.From_App,Fixed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' RL environment class'''\n",
    "class EnviromentsRL:\n",
    "    def __init__(self, opts):\n",
    "        if opts.Load_Checkpoints:\n",
    "            SC=opts.SC\n",
    "            if opts.VF_S==0 and opts.From_App: #If the user wants to set a final volume fraction, set the intermediate volume fractions accordingly\n",
    "                Vol_Frac_2=opts.Vol_Frac_2\n",
    "                Vol_Frac_1=opts.Vol_Frac_1\n",
    "                Vol_Frac_3=opts.Vol_Frac_3 \n",
    "        else:\n",
    "            Vol_Frac_3=opts.Vol_Frac_3\n",
    "            Vol_Frac_1=opts.Vol_Frac_1\n",
    "            Vol_Frac_2=opts.Vol_Frac_2\n",
    "        self.env = TopOpt_Gen(opts.Main_EX,opts.Main_EY,Vol_Frac_3,SC,opts)\n",
    "        self.env_primer= TopOpt_Gen(opts.PR_EX,opts.PR_EY,Vol_Frac_1,SC,opts)\n",
    "        self.env_primer2=TopOpt_Gen(opts.PR2_EX,opts.PR2_EY,Vol_Frac_2,SC,opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Class for Topology Optimization Options\n",
    "'''\n",
    "class Top_Options:\n",
    "    def __init__(self, Main_EX=24, Main_EY=24, PR2_EX=12, PR2_EY=12, PR_EX=6, PR_EY=6, Lx=1, Ly=1, Eta=2, a=5, b=5, replace=100, epsilon_dec=3.5e-4, eps_end=0.01, mem_size=30000, n_games=50000, batch_size=128, lr=5e-3, gamma=0.1, Vol_Frac_1=0.7, Vol_Frac_2=0.5, Vol_Frac_3=0.25, SC=10, P_Norm=10, filename_save='DDQN_TopOpt_Generalized_CNN_4L', filename_load='DDQN_TopOpt_Generalized_CNN_4L_6by6', Progressive_Refinement=True, LC=False, Load_Checkpoints=True, VF_S=0, Min_Dist=0, Time_Trial=True, configfile='config.json', From_App=True, base_folder=\".\"):\n",
    "        self.Main_EX = Main_EX  # Number of X Elements for Larger Environment\n",
    "        self.Main_EY = Main_EY  # Number of Y Elements for Larger Environment\n",
    "        self.PR2_EX = PR2_EX  # Number of X Elements for Second Environment used in Case of Progressive Refinement\n",
    "        self.PR2_EY = PR2_EY  # Number of Y Elements for Second Environment used in Case of Progressive Refinement\n",
    "        self.PR_EX = PR_EX  # Number of X Elements for Smaller Environment used in Case of Progressive Refinement\n",
    "        self.PR_EY = PR_EY  # Number of Y Elements for Smaller Environment used in Case of Progressive Refinement\n",
    "        self.Lx = Lx  # Length of the Structure in the X Direction\n",
    "        self.Ly = Ly  # Length of the Structure in the Y Direction\n",
    "        self.Eta = Eta  # Used for dynamic adjusting reward function. Larger eta means less prevalence given towards changes between current and previous reward. Recommend using [2,4]\n",
    "        self.a = a  # X Coefficient of the Quadratic Reward Surface\n",
    "        self.b = b  # Y Coefficient of the Quadratic Reward Surface\n",
    "        self.replace = replace  # Number of iterations between switching the weights from the active network to the target network\n",
    "        self.epsilon_dec = epsilon_dec  # Iterative decay amount of the epsilon value used for exploration/explotation\n",
    "        self.eps_end = eps_end  # Smallest Allowable Epsilon value to be used for exploration/explotation\n",
    "        self.mem_size = mem_size  # Size of the Replay Buffer\n",
    "        self.n_games = n_games  # Maximum Number of Training Episodes Conducted\n",
    "        self.batch_size = batch_size  # Batch Size that will be taken from the Replay Buffer per training episode\n",
    "        self.lr = lr  # Starting Learning Rate for the Network\n",
    "        self.gamma = gamma  # Discount Factor for Future Rewards\n",
    "        self.Vol_Frac_1 = Vol_Frac_1  # Volume Fraction during first progressive refinement\n",
    "        self.Vol_Frac_2 = Vol_Frac_2  # Final Volume Fraction\n",
    "        self.Vol_Frac_3 = Vol_Frac_3  # Final Volume Fraction\n",
    "        self.SC = SC  # Stress constraint, between 0 and 2\n",
    "        self.P_Norm = P_Norm  # Smoothing Parameter for P-Norm Global Stress calculation\n",
    "        self.filename_save = filename_save  # When training, what name would you like your weights, and figure saved as\n",
    "        self.filename_load = filename_load  # When testing, what name is your NN weights saved under\n",
    "        self.Progressive_Refinement = Progressive_Refinement\n",
    "        self.LC = LC # type in loading conditions manually\n",
    "        self.Load_Checkpoints = Load_Checkpoints\n",
    "        self.VF_S = VF_S # Use vol fraction constraint [0] or stress constraint [1]\n",
    "        self.Min_Dist = Min_Dist # The 0 value serves as a place holder to represent the minimum distance between the bounded and loaded elements in a given load case\n",
    "        self.Time_Trial = Time_Trial # Perform Time Trial\n",
    "        self.configfile = configfile # name of config file. \n",
    "        self.From_App = From_App # True if being called by an external app. Not sure this is needed. \"\n",
    "        self.base_folder = base_folder # Folder where to find saved files. Helpful if not running the app from the main folder. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Top_Options()\n",
    "User_Conditions = json.load(open(opts.configfile) ) if opts.From_App else None  \n",
    "opts = Top_Options()\n",
    "envs = EnviromentsRL(opts)  \n",
    "App_Plot=TopOpt_Designing(User_Conditions,opts, envs)\n",
    "json.dump( App_Plot, open( \"App_Data.json\", 'w' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layout(file_name='App_Data.json', number_of_elements=(opts.Main_EX, opts.Main_EY), save_name='App_Plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.configfile = \"../ksme2023-config/config_jason_files/config_fig_13(a).json\"\n",
    "User_Conditions = json.load(open(opts.configfile) ) if opts.From_App else None  \n",
    "opts = Top_Options()\n",
    "envs = EnviromentsRL(opts)  \n",
    "App_Plot=TopOpt_Designing(User_Conditions,opts, envs)\n",
    "json.dump( App_Plot, open( \"App_Data2.json\", 'w' ) )\n",
    "plot_layout(file_name='App_Data2.json', number_of_elements=(opts.Main_EX, opts.Main_EY), save_name='App_Plot2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.configfile = \"../ksme2023-config/config_jason_files/config_table_1(b).json\"\n",
    "User_Conditions = json.load(open(opts.configfile) ) if opts.From_App else None  \n",
    "opts = Top_Options()\n",
    "envs = EnviromentsRL(opts)  \n",
    "App_Plot=TopOpt_Designing(User_Conditions,opts, envs)\n",
    "json.dump( App_Plot, open( \"App_Data3.json\", 'w' ) )\n",
    "plot_layout(file_name='App_Data3.json', number_of_elements=(opts.Main_EX, opts.Main_EY), save_name='App_Plot3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.configfile = \"../ksme2023-config/config_jason_files/config_table_1(c).json\"\n",
    "User_Conditions = json.load(open(opts.configfile) ) if opts.From_App else None  \n",
    "opts = Top_Options()\n",
    "envs = EnviromentsRL(opts)  \n",
    "App_Plot=TopOpt_Designing(User_Conditions,opts, envs)\n",
    "json.dump( App_Plot, open( \"App_Data4.json\", 'w' ) )\n",
    "plot_layout(file_name='App_Data4.json', number_of_elements=(opts.Main_EX, opts.Main_EY), save_name='App_Plot4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.configfile = \"../ksme2023-config/config_jason_files/config_table_1(d).json\"\n",
    "User_Conditions = json.load(open(opts.configfile) ) if opts.From_App else None  \n",
    "opts = Top_Options()\n",
    "envs = EnviromentsRL(opts)  \n",
    "App_Plot=TopOpt_Designing(User_Conditions,opts, envs)\n",
    "json.dump( App_Plot, open( \"App_Data42.json\", 'w' ) )\n",
    "plot_layout(file_name='App_Data42.json', number_of_elements=(opts.Main_EX, opts.Main_EY), save_name='App_Plot42.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.configfile = \"../ksme2023-config/config_jason_files/config_table_1(c).json\"\n",
    "opts.PR_EX = 30\n",
    "User_Conditions = json.load(open(opts.configfile) ) if opts.From_App else None  \n",
    "opts = Top_Options()\n",
    "envs = EnviromentsRL(opts)  \n",
    "App_Plot=TopOpt_Designing(User_Conditions,opts, envs)\n",
    "json.dump( App_Plot, open( \"App_Data43.json\", 'w' ) )\n",
    "plot_layout(file_name='App_Data43.json', number_of_elements=(opts.Main_EX, opts.Main_EY), save_name='App_Plot43.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ksme2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2d8cee5fdffea191b801dc9e68000897c9e94bb15de5fb178821519dcf1f383"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
